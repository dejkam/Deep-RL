{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anatomy of the RL Components:\n",
    "\n",
    "Agent: An entity that observe the states of the envirobment and takes an action based on the current state\n",
    "and a policy. In practice, the agent is a piece of code that implements the policy and decide what action\n",
    "is needed at every time step, given the observations.\n",
    "\n",
    "Environment: some model of the world, which is external to the agent and has the responsibility of providing the agents with observations and giving them rewards. It changes its state based on the actions done by the agents.\n",
    "\n",
    "The following excerpts from the book represent important basic concepts about the RL model. \n",
    "The environment could be an extremely complicated physics model, and an agent could easily be a large neural network implementing the latest RL algorithm.\n",
    "However, the basic pattern stays the same: on every step, an agent takes some observations from the environment, does its calculations, and selects the action to issue. The result of this action is a reward and new observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of en environment that gives the agent randomo rewards for a limited number of steps,\n",
    "# regardless of the agent's actions\n",
    "\n",
    "class Environment:\n",
    "    # initializing the envirobment's internal state\n",
    "    def __init__(self):\n",
    "        # a counter thta limits the number of time steps the agent is allowed to interact with the env.\n",
    "        self.steps_left = 10\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Return the current state of the environment\n",
    "        \"\"\"\n",
    "        return [0.0, 0.0, 0.0]\n",
    "\n",
    "    def get_actions(self):\n",
    "        \"\"\"\n",
    "        Allows the agents to query the set of actions it can execute. \n",
    "        Twon actions are allows in this scenario and are encoded with the integers 0 and 1.\n",
    "        \"\"\"\n",
    "        return [0,1]\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        This method signals the end of the episode to the agent.\n",
    "        \"\"\"\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def action(self, action):\n",
    "        \"\"\"\n",
    "        The central Piece in the environment's functionality:\n",
    "        - Handles the agent's action\n",
    "        - Returns the reward for the action\n",
    "        In this example the reward is random and the action is discarded.\n",
    "        \"\"\"\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random()\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def step(self, env):\n",
    "        \"\"\"\n",
    "        The step function accepts the environment instance and allows the agent to do:\n",
    "        - Observe the environment\n",
    "        - Maka a decision about the action to take based on the observaions\n",
    "        - Submit the action to the environment\n",
    "        - Get the reward for the current step\n",
    "        \"\"\"\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward is: 4.5876\n"
     ]
    }
   ],
   "source": [
    "# The glue code to create both classes and run one episode\n",
    "if __name__ == \"__main__\":\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "    \n",
    "    while not env.is_done():\n",
    "        agent.step(env)\n",
    "    \n",
    "    print(f'Total reward is: {agent.total_reward:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GYM",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
